Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Lindstrom2011,
abstract = {But most striking of all was the flurry of activation in the insular cortex of the brain, which is associated with feelings of love and compassion. The subjects' brains responded to the sound of their phones as they would respond to the presence or proximity of a girlfriend, boyfriend or family member. In short, the subjects didn't demonstrate the classic brain-based signs of addiction. Instead, they loved their iPhones.},
author = {Lindstrom, Martin},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Lindstrom - 2011 - You Love Your iPhone. Literally.html:html},
issn = {0362-4331},
journal = {The New York Times},
pages = {21A},
publisher = {The New York Times},
title = {{You Love Your iPhone. Literally.}},
url = {http://www.nytimes.com/2011/10/01/opinion/you-love-your-iphone-literally.html},
year = {2011}
}
@article{Goel2012,
abstract = {We introduce GripSense, a system that leverages mobile device touchscreens and their built-in inertial sensors and vibration motor to infer hand postures including one- or two-handed interaction, use of thumb or index finger, or use on a table. GripSense also senses the amount of pres-sure a user exerts on the touchscreen despite a lack of direct pressure sensors by inferring from gyroscope readings when the vibration motor is "pulsed." In a controlled study with 10 participants, GripSense accurately differentiated device usage on a table vs. in hand with 99.67{\%} accuracy and when in hand, it inferred hand postures with 84.26{\%} accuracy. In addition, GripSense distinguished three levels of pressure with 95.1{\%} accuracy. A usability analysis of GripSense was conducted in three custom applications and showed that pressure input and hand-posture sensing can be useful in a number of scenarios.},
author = {Goel, Mayank and Wobbrock, Jacob O and Patel, Shwetak N},
doi = {10.1145/2380116.2380184},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Goel, Wobbrock, Patel - 2012 - GripSense.pdf:pdf},
isbn = {9781450315807},
journal = {Proceedings of the 25th annual ACM symposium on User interface software and technology - UIST '12},
keywords = {a user to perform,figure 1,gripsense senses,hand,infers pressure exerted on,interactions,it is difficult for,left,like pinch-to-zoom with one,mobile,right,s hand posture and,situational impairments,the screen,touchscreen,user},
pages = {545},
title = {{GripSense}},
url = {http://dl.acm.org/citation.cfm?doid=2380116.2380184{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=2380184{\&}ftid=1294299{\&}dwn=1{\&}CFID=216938597{\&}CFTOKEN=33552307},
year = {2012}
}
@article{Dan-glauser2011,
author = {Dan-glauser, Elise S and Scherer, Klaus R},
doi = {10.3758/s13428-011-0064-1},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Dan-glauser, Scherer - 2011 - The Geneva affective picture database ( GAPED ) a new 730-picture database focusing on valence and normati.pdf:pdf},
keywords = {dan-glauser,e,emotion induction,k,normative significance,picture database,r,s,scherer,snakes,spiders,university of geneva},
pages = {468--477},
title = {{The Geneva affective picture database ( GAPED ): a new 730-picture database focusing on valence and normative significance}},
year = {2011}
}
@article{Fontaine2007,
abstract = {For more than half a century, emotion researchers have attempted to establish the dimensional space that most economically accounts for similarities and differences in emotional experience. Today, many researchers focus exclusively on two-dimensional models involving valence and arousal. Adopting a theoretically based approach, we show for three languages that four dimensions are needed to satisfactorily represent similarities and differences in the meaning of emotion words. In order of importance, these dimensions are evaluationpleasantness, potency-control, activation-arousal, and unpredictability. They were identified on the basis of the applicability of 144 features representing the six components of emotions: (a) appraisals of events, (b) psychophysiological changes, (c) motor expressions, (d) action tendencies, (e) subjective experiences, and (f) emotion regulation.},
author = {Fontaine, Johnny R J and Scherer, Klaus R and Roesch, Etienne B and Phoebe, C and Fontaine, Johnny R J and Scherer, Klaus R and Roesch, Etienne B and Ellsworth, Phoebe C},
doi = {10.1111/j.1467-9280.2007.02024.x},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Fontaine et al. - 2007 - The World of Emotions Is Not Two-Dimensional.pdf:pdf},
number = {12},
pages = {1050--1057},
pmid = {18031411},
title = {{The World of Emotions Is Not Two-Dimensional}},
url = {http://dx.doi.org/10.1111/j.1467-9280.2007.02024.x},
volume = {18},
year = {2007}
}
@article{Singh2015,
abstract = {In this article, we describe the Differential Equations and Optimization Environment (DOpElib). DOpElib is a software library that provides a unified interface to high level algorithms such as time-stepping methods, nonlinear solvers and optimization routines. This structure ensures that, first of all, the user is only required to write those sections of code that are specific to the considered problem. Second, the exchange of parts of the used routines is possible with only a few lines of code to change instead of large reimplementations. The article illustrates the design principles and various features of DOpElib and provides some numerical results as demonstration for the versatility of the software},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.07526v1},
author = {Singh, Munindar P.},
doi = {10.1145/0000000.0000000},
eprint = {arXiv:1502.07526v1},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
number = {March},
pages = {4207--4211},
pmid = {1000285845},
title = {{Norms as a basis for governing sociotechnical systems}},
volume = {2015-Janua},
year = {2015}
}
@article{Likamwa2013,
abstract = {We report a first-of-its-kind smartphone software system, MoodScope, which infers the mood of its user based on how the smartphone is used. Compared to smartphone sensors that measure acceleration, light, and other physical properties, MoodScope is a "sensor" that measures the mental state of the user and provides mood as an important input to context-aware computing. We run a formative statistical mood study with smartphone-logged data collected from 32 participants over two months. Through the study, we find that by analyzing communication history and application usage patterns, we can statistically infer a user's daily mood average with an initial accuracy of 66{\%}, which gradu-ally improves to an accuracy of 93{\%} after a two-month personal-ized training period. Motivated by these results, we build a service, MoodScope, which analyzes usage history to act as a sensor of the user's mood. We provide a MoodScope API for developers to use our system to create mood-enabled applications. We further create and deploy a mood-sharing social application.},
author = {Likamwa, Robert and Liu, Yunxin and Lane, Nicholas D. and Zhong, Lin},
doi = {10.1145/2462456.2464449},
isbn = {9781450316729},
journal = {MobiSys '13 Proceeding of the 11th annual international conference on Mobile systems, applications, and services},
keywords = {affective computing,machine learning,mood,smartphone usage},
number = {April},
pages = {389--402},
title = {{MoodScope: Building a Mood Sensor from Smartphone Usage Patterns}},
year = {2013}
}
@article{Lang1997,
abstract = {This paper proposes two new methodologies for the placement of series FACTS devices in deregulated electricity market to reduce congestion. Similar to sensitivity factor based method, the proposed methods form a priority list that reduces the solution space. The proposed methodologies are based on the use of LMP differences and congestion rent, respectively. The methods are computationally efficient, since LMPs are the by-product of a security constrained OPF and congestion rent is a function of LMP difference and power flows. The proposed methodologies are tested and validated for locating TCSC in IEEE 14-, IEEE 30- and IEEE 57-bus test systems. Results obtained with the proposed methods are compared with that of the sensitivity method and with exhaustive OPF solutions. The overall objective of FACTS device placement can be either to minimize the total congestion rent or to maximize the social welfare. Results show that the proposed methods are capable of finding the best location for TCSC installation, that suite both objectives. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {0005-7916(93)E0016-Z},
author = {Lang, P.J. and Bradley, M.M. and Cuthbert, B.N.},
doi = {10.1027/0269-8803/a000147},
eprint = {0005-7916(93)E0016-Z},
file = {:Users/kevin/Downloads/instructions.pdf:pdf},
isbn = {Technical Report A-4},
issn = {0269-8803},
journal = {NIMH Center for the Study of Emotion and Attention},
keywords = {Congestion rent,LMP,OPF,TCSC},
pages = {39--58},
pmid = {8625375},
title = {{International Affective Picture System (IAPS): Technical Manual and Affective Ratings}},
url = {http://www.unifesp.br/dpsicobio/adap/instructions.pdf{\%}5Cnhttp://econtent.hogrefe.com/doi/abs/10.1027/0269-8803/a000147},
year = {1997}
}
@article{Shah2015,
abstract = {The role of affect and emotion in interactive system design is an active and recent research area. The aim is to make systems more responsive to user's needs and expectations. The first step towards affective interaction is to recognize user's emotional state. Literature contains many works on emotion recognition. In those works, facial muscle movement, gestures, postures and physiological signals were used for recognition. The methods are computation intensive and require extra hardware (e.g., sensors and wires). In this work, we propose a simpler model to predict the affective state of a touch screen user. The prediction is done based on the user's touch input, namely the finger strokes. We defined seven features based on the strokes. A linear combination of these features is proposed as the predictor, which can predict a user's affective state into one of the three states: positive (happy, excited and elated), negative (sad, anger, fear, disgust) and neutral (calm, relaxed and contented). The model alleviates the need for extra setup as well as extensive computation, making it suitable for implementation on mobile devices with limited resources. The model is developed and validated with empirical data involving 57 participants performing 7 touch input tasks. The validation study demonstrates a high prediction accuracy of 90.47 {\%}. The proposed model and its empirical development and validation are described in this paper.},
author = {Shah, Sachin and Teja, J. Narasimha and Bhattacharya, Samit},
doi = {10.1186/s40166-015-0013-z},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Shah, Teja, Bhattacharya - 2015 - Towards affective touch interaction predicting mobile user emotion from finger strokes.pdf:pdf},
issn = {2194-0827},
journal = {Journal of Interaction Science},
keywords = {emotional state touch screen,linear regression empirical study,strike and tap features},
number = {1},
pages = {6},
publisher = {Journal of Interaction Science},
title = {{Towards affective touch interaction: predicting mobile user emotion from finger strokes}},
url = {http://www.journalofinteractionscience.com/content/3/1/6},
volume = {3},
year = {2015}
}
@article{Grunerbl2015,
abstract = {Today's health care is difficult to imagine without the possibility to objectively measure various physiological parameters related to patients symptoms (from temperature through blood pressure to complex tomographic procedures). Psychiatric care remains a notable exception that heavily relies on patient interviews and self assessment. This is due to the fact that mental illnesses manifest themselves mainly in the way patients behave throughout their daily life and, until recently there were no "behavior measurement devices". This is now changing with the progress in wearable activity recognition and sensor enabled smartphones. In this article we introduce a system, which, based on smartphone-sensing is able to recognize depressive and manic states and detect state changes of patients suffering from bipolar disorder. Drawing upon a real-life dataset of 10 patients, recorded over a time-period of 12 weeks (in total over 800 days of data tracing 17 state changes) by 4 different sensing modalities we could extract features corresponding to all disease-relevant aspects in behavior. Using these features we gain recognition accuracies of 76{\%} by fusing all sensor modalities and state change detection precision and recall of over 97{\%}. This article furthermore outlines the applicability of this system in the physician-patient relations in order to facilitate the life and treatment of bipolar patients.},
author = {Gr{\"{u}}nerbl, Agnes and Muaremi, Amir and Osmani, Venet and Bahle, Gernot and {\"{O}}hler, Stefan and Tr{\"{o}}ster, Gerard and Mayora, Oscar and Haring, Christian and Lukowicz, Paul},
doi = {10.1109/JBHI.2014.2343154},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Gr{\"{u}}nerbl et al. - 2015 - Smart-Phone Based Recognition of States and State Changes in Bipolar Disorder Patients.pdf:pdf},
isbn = {2168-2194 VO - 19},
issn = {2168-2208},
journal = {IEEE Journal of Biomedical and Health Informatics},
number = {1},
pages = {140--148},
pmid = {25073181},
title = {{Smart-Phone Based Recognition of States and State Changes in Bipolar Disorder Patients}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25073181},
volume = {19},
year = {2015}
}
@misc{Ekman1992,
abstract = {Emotions are viewed as having evolved through their adaptive value in dealing with fundamental life-tasks. Each emotion has unique features: signal, physiology, and antecedent events. Each emotion also has characteristics in common with other emotions: rapid onset, short duration, unbidden occurrence, automatic appraisal, and coherence among responses. These shared and unique characteristics are the product of our evolution, and distinguish emotions from other affective pheonomena.},
archivePrefix = {arXiv},
arxivId = {a},
author = {Ekman, Paul},
booktitle = {Cognition {\&} Emotion},
doi = {10.1080/02699939208411068},
eprint = {a},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Ekman - 1992 - An argument for basic emotions.pdf:pdf},
isbn = {0269-9931},
issn = {0269-9931},
number = {3},
pages = {169--200},
pmid = {665},
title = {{An argument for basic emotions}},
volume = {6},
year = {1992}
}
@article{Lee2012,
abstract = {Awareness of the emotion of those who communicate with others is a fundamental challenge in building affective intelligent systems. Emotion is a complex state of the mind influenced by external events, physiological changes, or relationships with others. Because emotions can represent a user's internal context or intention, researchers suggested various methods to measure the user's emotions from analysis of physiological signals, facial expressions, or voice. However, existing methods have practical limitations to be used with consumer devices, such as smartphones; they may cause inconvenience to users and require special equipment such as a skin conductance sensor. Our approach is to recognize emotions of the user by inconspicuously collecting and analyzing user-generated data from different types of sensors on the smartphone. To achieve this, we adopted a machine learning approach to gather, analyze and classify device usage patterns, and developed a social network service client for Android smartphones which unobtrusively find various behavioral patterns and the current context of users. Also, we conducted a pilot study to gather real-world data which imply various behaviors and situations of a participant in her/his everyday life. From these data, we extracted 10 features and applied them to build a Bayesian Network classifier for emotion recognition. Experimental results show that our system can classify user emotions into 7 classes such as happiness, surprise, anger, disgust, sadness, fear, and neutral with a surprisingly high accuracy. The proposed system applied to a smartphone demonstrated the feasibility of an unobtrusive emotion recognition approach and a user scenario for emotion-oriented social communication between users.},
author = {Lee, Hosub and Choi, Young Sang and Lee, Sunjae and Park, I. P.},
doi = {10.1109/CCNC.2012.6181098},
isbn = {9781457720710},
journal = {2012 IEEE Consumer Communications and Networking Conference, CCNC'2012},
keywords = {Affective computing,Computer mediated communication,Emotion recognition,Machine intelligence,Supervised learning},
pages = {260--264},
title = {{Towards unobtrusive emotion recognition for affective social communication}},
year = {2012}
}
@article{Posner2005,
abstract = {The circumplex model of affect proposes that all affective states arise from cognitive interpretations of core neural sensations that are the product of two independent neurophysiological systems. This model stands in contrast to theories of basic emotions, which posit that a discrete and independent neural system subserves every emotion. We propose that basic emotion theories no longer explain adequately the vast number of empirical observations from studies in affective neuroscience, and we suggest that a conceptual shift is needed in the empirical approaches taken to the study of emotion and affective psychopathologies. The circumplex model of affect is more consistent with many recent findings from behavioral, cognitive neuroscience, neuroimaging, and developmental studies of affect. Moreover, the model offers new theoretical and empirical approaches to studying the development of affective disorders as well as the genetic and cognitive underpinnings of affective processing within the central nervous system.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Posner, Jonathan and Russell, James A and Peterson, Bradley S},
doi = {10.1017/S0954579405050340},
eprint = {NIHMS150003},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Posner, Russell, Peterson - 2005 - The circumplex model of affect an integrative approach to affective neuroscience, cognitive developme.pdf:pdf},
isbn = {0954-5794 (Print) 0954-5794 (Linking)},
issn = {0954-5794},
journal = {Development and psychopathology},
number = {3},
pages = {715--34},
pmid = {16262989},
title = {{The circumplex model of affect: an integrative approach to affective neuroscience, cognitive development, and psychopathology.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16262989},
volume = {17},
year = {2005}
}
@article{Plutchik1980,
abstract = {What is an emotion? More than 90 definitions have been offered over the past century, and there are almost as many theories of emotion—not to mention a complex array of overlapping words in our languages to describe them. Plutchik offers an integrative theory based on evolutionary principles. Emotions are adaptive—in fact, they have a complexity born of a long evolutionary history--and although we conceive of emotions as feeling states, Plutchik says the feeling state is part of a process involving both cognition and behavior and containing several feedback loops.},
author = {Plutchik, Robert},
journal = {American Scientist},
number = {February},
pages = {2007},
title = {{Psychoevolutionary Theory of Basic Emotions}},
url = {http://web.archive.org/web/20010716082847/http://americanscientist.org/articles/01articles/Plutchik.html},
year = {1980}
}
@article{Hasson2009,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hasson, Christopher J and Caldwell, Graham E and Emmerik, Richard E A Van},
doi = {10.1016/j.humov.2008.02.015.Changes},
eprint = {NIHMS150003},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Hasson, Caldwell, Emmerik - 2009 - NIH Public Access.pdf:pdf},
isbn = {3300000106},
issn = {09652140},
journal = {Motor Control},
keywords = {biarticular muscles,coordination,force directing,learning,monoarticular muscles},
number = {4},
pages = {590--609},
pmid = {20402989},
title = {{NIH Public Access}},
volume = {27},
year = {2009}
}
@article{Wioleta2013,
abstract = {review},
author = {Wioleta, Szwoch},
doi = {10.1109/HSI.2013.6577880},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Wioleta - 2013 - Using physiological signals for emotion recognition.pdf:pdf},
isbn = {9781467356374},
issn = {2158-2246},
journal = {Human System Interaction (HSI), 2013 The 6th {\ldots}},
keywords = {4,affective computing,applications of emotion recognition,emotion recognition,physiological signals,possible,so there are many,their emotions explicitly,using physiological},
pages = {556--561},
title = {{Using physiological signals for emotion recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6577880},
year = {2013}
}
@book{Scherer2000,
abstract = {This chapter provides an overview of theories currently in the psychology of emotion and the controversies and research issues they generate. As should become obvious in this review, many of the fundamental differences among the models relate to the thorny issue of the definition of the phenomenon called emotion and its conceptualization and operationalization. Not surprisingly, the disagreement as to the nature of emotion extends to the problem of delimitation of the psychological states or processes to be studied under this label from other affective phenomena. We first review the elements of the definition of emotion that seem to show at least some degree of convergence between different theorists.},
author = {Scherer, Klaus R.},
booktitle = {The neuropsychology of emotion},
doi = {10.1097/00001504-198811000-00012},
isbn = {0-19-511464-7 (Hardcover)},
issn = {0951-7367},
pages = {137--162},
title = {{Psychological models of emotion}},
year = {2000}
}
@article{Gao2012,
abstract = {The increasing number of people playing games on touch-screen mobile phones raises the question of whether touch behaviors reflect players' emotional states. This prospect would not only be a valuable eval- uation indicator for game designers, but also for real-time personalization of the game experience. Psychol- ogy studies on acted touch behavior show the existence of discriminative affective profiles. In this article, finger-stroke features during gameplay on an iPod were extracted and their discriminative power analyzed. Machine learning algorithms were used to build systems for automatically discriminating between four emo- tional states (Excited, Relaxed, Frustrated, Bored), two levels of arousal and two levels of valence. Accuracy reached between 69{\%} and 77{\%} for the four emotional states, and higher results (∼89{\%}) were obtained for discriminating between two levels of arousal and two levels of valence. We conclude by discussing the factors relevant to the generalization of the results to applications other than games.},
author = {Gao, Yuan and Bianchi-Berthouze, Nadia and Meng, Hongying},
doi = {10.1145/2395131.2395138},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Gao, Bianchi-Berthouze, Meng - 2012 - What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay.pdf:pdf},
isbn = {10730516 (ISSN)},
issn = {10730516},
journal = {ACM Transactions on Computer-Human Interaction},
number = {4},
pages = {1--30},
title = {{What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay?}},
url = {http://dl.acm.org/citation.cfm?doid=2395131.2395138},
volume = {19},
year = {2012}
}
@article{Coulson2004,
abstract = {ABSTRACT: A total of 176 computer-generated mannequin figures were produced from descriptions of postural expressions of emotion in order to investigate the attri- bution of emotion to static body postures. Each posture was rendered from 3 view- ing angles and presented to participants in a forced-decision task. Concordance rates for attributions of 6 emotions (anger, disgust, fear, happiness, sadness, and surprise) ranged from zero for many disgust postures to over 90 percent for some anger and sadness postures. Anatomical variables and viewing angle were shown to predict participants' responses. Analysis of the confusion matrix suggested a cir- cumplex solution with happiness and surprise sharing a similar position, and few confusions between the other four emotions. The means by which emotions may be attributed to static body postures are discussed, as are avenues for further research.},
author = {Coulson, Mark},
doi = {10.1023/B:JONB.0000023655.25550.be},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Coulson - 2004 - Attributing emotion to static body postures recongition accuracy, confusion and view point dependence.pdf:pdf},
isbn = {0191-5886},
issn = {0191-5886},
journal = {Journal of Nonverbal Behavior},
keywords = {and,and communication of emo-,emotion,expression,facial and vocal expression,posture,posture in the expression,relation of research into,s the expression of,since publication of darwin,the emotions in man,the role of body,tion remains the poor},
number = {2},
pages = {117--139},
pmid = {4869},
title = {{Attributing emotion to static body postures: recongition accuracy, confusion and view point dependence}},
volume = {28},
year = {2004}
}
@article{FeldmanBarrett1998,
abstract = {The independence of positive and negative affect has been heralded as a major and counterintuitive finding in the psychology of mood and emotion. Still, other findings support the older view that positive and negative fall at opposite ends of a single bipolar continuum. Independence versus bipolarity can be reconciled by considering (a) the activation dimension of affect, (b) random and systematic measurement error, and (c) how items are selected to achieve an appropriate test of bipolarity. In 3 studies of self-reported current affect, random and systematic error were controlled through multiformat measurement and confirmatory factor analysis. Valence was found to be indepen- dent of activation, positive affect the bipolar opposite of negative affect, and deactivation the bipolar opposite of activation. The dimensions underlying D. Watson, L. A. Clark, and A. Tellegen's (1988) Positive and Negative Affect schedule were accounted for by the valence and activation dimensions.},
author = {{Feldman Barrett}, Lisa and Russell, James A},
doi = {10.1037/0022-3514.74.4.967},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Feldman Barrett, Russell - 1998 - Independence and bipolarity in structure of current affect.pdf:pdf},
isbn = {0022-3514, 0022-3514},
issn = {0022-3514},
journal = {Journal of Personality and Social Psychology},
keywords = {affect},
number = {4},
pages = {967--984},
pmid = {8476717051535740061},
title = {{Independence and bipolarity in structure of current affect}},
volume = {74},
year = {1998}
}
@article{Zimmermann2003,
abstract = {Emotions are an increasingly important factor in Human-Computer Interaction (HCI). Up to the present, emotion recognition in HCI implies the use of explicit or intrusive methods, for example, video cameras or physiological measurements. We are developing and evaluating a method for the measurement of affective states through motor-behavioral parameters from standard input devices (mouse and keyboard).},
author = {Zimmermann, Philippe and Guttormsen, Sissel and Danuser, Brigitta and Gomez, Patrick},
doi = {10.1080/10803548.2003.11076589},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Zimmermann et al. - 2003 - Affective computing—a rationale for measuring mood with mouse and keyboard.pdf:pdf},
isbn = {1080-3548 (Print)$\backslash$r1080-3548 (Linking)},
issn = {10803548},
journal = {International Journal of Occupational Safety and Ergonomics},
keywords = {Affective computing,Emotions,HCI,Mood},
number = {4},
pages = {539--551},
pmid = {14675525},
title = {{Affective computing—a rationale for measuring mood with mouse and keyboard}},
volume = {9},
year = {2003}
}
@article{LiuJ2007,
author = {Liu, Jia and Chen, Chun and Bu, Jiajun and You, Mingyu and Tao, Jianhua},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2007 - SPEECH EMOTION RECOGNITION USING AN ENHANCED CO-TRAINING ALGORITHM Jia Liu , Chun Chen , Jiajun Bu , Mingyu You Coll.pdf:pdf},
isbn = {1424410177},
pages = {999--1002},
title = {{SPEECH EMOTION RECOGNITION USING AN ENHANCED CO-TRAINING ALGORITHM Jia Liu , Chun Chen , Jiajun Bu , Mingyu You College of Computer Science Zhejiang University National Lab of Pattern Recognition Chinese Academy of Sciences}},
year = {2007}
}
@article{Silva1997,
author = {Silva, Liyanage C D E and Miyasato, I Tsutomu},
doi = {10.1109/SMC.2015.387},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Silva, Miyasato - 1997 - Facial Emotion Recognition Using.pdf:pdf},
isbn = {0780336763},
number = {September},
pages = {9--12},
title = {{Facial Emotion Recognition Using}},
year = {1997}
}
@article{Essl2010,
author = {Essl, Georg and Rohs, Michael and Kratz, Sven},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Essl, Rohs, Kratz - 2010 - Use the Force (or something)-Pressure and Pressure-Like Input for Mobile Music Performance.pdf:pdf},
journal = {NIME 2010 Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {force,impact,mobile,mobile phone,multi-touch,pressure},
number = {Nime},
pages = {182--185},
title = {{Use the Force (or something)-Pressure and Pressure-Like Input for Mobile Music Performance}},
url = {http://141.84.8.93/pubdb/publications/pub/essl2010pressuremusic/essl2010pressuremusic.pdf{\%}5Cnhttp://www.nime.org/proceedings/2010/nime2010{\_}182.pdf},
year = {2010}
}
@article{Picard1995,
abstract = {Computers are beginning to acquire the ability to ex- press and recognize affect, and may soon be given the ability to “have emotions.” The essential role of emotion in both human cognition and perception, as demonstrated by recent neurological studies, indi- cates that affective computers should not only pro- vide better performance in assisting humans, but also might enhance computers' abilities to make de- cisions. This paper presents and discusses key issues in “affective computing,” computing that relates to, arises from, or influences emotions. Models are sug- gested for computer recognition of human emotion, and new applications are presented for computer- assisted learning, perceptual information retrieval, arts and entertainment, and human health and inter- action. Affective computing, coupled with new wear- able computers, will also provide the ability to gather new data necessary for advances in emotion and cog- nition theory.},
author = {Picard, Rosalind W.},
doi = {10.1007/BF01238028},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Picard - 1995 - Affective Computing.pdf:pdf},
isbn = {0262161702},
issn = {09269630},
journal = {MIT press},
number = {321},
pages = {1--16},
pmid = {18487783},
title = {{Affective Computing}},
url = {papers3://publication/uuid/9C02FCAE-FE2E-4D2C-9707-766804777DC9},
year = {1995}
}
@article{Ververidis2004,
abstract = {Our purpose is to design a useful tool which can be used in psy- chology to automatically classify utterances into five emotional states such as anger, happiness, neutral, sadness, and surprise. The major contribution of the paper is to rate the discriminating capa- bility of a set of features for emotional speech recognition. A total of 87 features has been calculated over 500 utterances from the Danish Emotional Speech database. The Sequential Forward Se- lection method (SFS) has been used in order to discover a set of 5 to 10 features which are able to classify the utterances in the best way. The criterion used in SFS is the crossvalidated correct clas- sification score of one of the following classifiers: nearest mean and Bayes classifier where class pdfs are approximated via Parzen windows or modelled as Gaussians. After selecting the 5 best fea- tures, we reduce the dimensionality to two by applying principal component analysis. The result is a 51.6{\%} ± 3{\%} correct classifi- cation rate at 95{\%} confidence interval for the five aforementioned emotions, whereas a random classification would give a correct classification rate of 20{\%}. Furthermore, we find out those two- class emotion recognition problems whose error rates contribute heavily to the average error and we indicate that a possible reduc- tion of the error rates reported in this paper would be achieved by employing two-class classifiers and combining them..},
author = {Ververidis, Dimitrios and Kotropoulos, Constantine and Ioannis, Pitas},
doi = {10.1109/ICASSP.2004.1326055},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Ververidis, Kotropoulos, Ioannis - 2004 - Automatic Emotional Speech Classification.pdf:pdf},
isbn = {0-7803-8484-9},
issn = {15206149},
journal = {Artificial Intelligence and Information Analysis Laboratory},
pages = {593-- 596},
title = {{Automatic Emotional Speech Classification}},
year = {2004}
}
@article{Hertenstein2006,
abstract = {The study of emotional signaling has focused almost exclusively on the face and voice. In 2 studies, the authors investigated whether people can identify emotions from the experience of being touched by a stranger on the arm (without seeing the touch). In the 3rd study, they investigated whether observers can identify emotions from watching someone being touched on the arm. Two kinds of evidence suggest that humans can communicate numerous emotions with touch. First, participants in the United States (Study 1) and Spain (Study 2) could decode anger, fear, disgust, love, gratitude, and sympathy via touch at much-better-than-chance levels. Second, fine-grained coding documented specific touch behaviors associated with different emotions. In Study 3, the authors provide evidence that participants can accurately decode distinct emotions by merely watching others communicate via touch. The findings are discussed in terms of their contributions to affective science and the evolution of altruism and cooperation.},
author = {Hertenstein, Matthew J. and Keltner, Dacher and App, Betsy and Bulleit, Brittany A. and Jaskolka, Ariane R.},
doi = {10.1037/1528-3542.6.3.528},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Hertenstein et al. - 2006 - Touch communicates distinct emotions.pdf:pdf},
isbn = {1528-3542 (Print)$\backslash$r1528-3542 (Linking)},
issn = {1931-1516},
journal = {Emotion},
keywords = {and,and childhood,and it contributes to,brain,cognitive,developed,emotion communication,human social life,it is the most,physical contact,sensory modality at birth,socioemotional development throughout infancy,tactile stimulation,touch,touch is central to},
number = {3},
pages = {528--533},
pmid = {16938094},
title = {{Touch communicates distinct emotions.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1528-3542.6.3.528},
volume = {6},
year = {2006}
}
@article{Mauss2009,
abstract = {A consensual, componential model of emotions conceptualises them as experiential, physiological, and behavioural responses to personally meaningful stimuli. The present review examines this model in terms of whether different types of emotion-evocative stimuli are associated with discrete and invariant patterns of responding in each response system, how such responses are structured, and if such responses converge across different response systems. Across response systems, the bulk of the available evidence favours the idea that measures of emotional responding reflect dimensions rather than discrete states. In addition, experiential, physiological, and behavioural response systems are associated with unique sources of variance, which in turn limits the magnitude of convergence across measures. Accordingly, the authors suggest that there is no "gold standard" measure of emotional responding. Rather, experiential, physiological, and behavioural measures are all relevant to understanding emotion and cannot be assumed to be interchangeable.},
author = {Mauss, Iris B and Robinson, Michael D},
doi = {10.1080/02699930802204677},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Mauss, Robinson - 2009 - Measures of emotion A review.pdf:pdf},
isbn = {1464-0600 (Electronic)$\backslash$r0269-9931 (Linking)},
issn = {1464-0600},
journal = {Cognition and Emotion},
keywords = {Autonomic nervous system,Behaviour,Central nervous system,Emotion,Measurement,Self-report,Specificity,Startle modulation},
number = {2},
pages = {209--237},
pmid = {19809584},
title = {{Measures of emotion: A review.}},
url = {http://www.tandfonline.com/doi/abs/10.1080/02699930802204677},
volume = {23},
year = {2009}
}
@article{Wallbott1998,
abstract = {The question whether body movements and body postures are indicative of specific emotions is a matter of debate. While some studies have found evidence for specific body movements accompanying specific emotions, others indicate that movement behavior (aside from facial expression) may be only indicative of the quantity (intensity) of emotion, but not of its quality. The study reported here is an attempt to demonstrate that body movements and postures to some degree are specific for certain emotions. A sample of 224 video takes, in which actors and actresses portrayed the emotions of elated joy, happiness, sadness, despair, fear, terror, cold anger, hot anger, disgust, contempt, shame, guilt, pride, and boredom via a scenario approach, was analyzed using coding schemata for the analysis of body movements and postures. Results indicate that some emotion-specific movement and posture characteristics seem to exist, but that for body movements dierences between emotions can be partly explained by the dimension of activation. While encoder (actor) diferences are rather pronounced with respect to specific movement and posture habits, these dierences are largely independent from the emotion-specific dierences found. The results are discussed with respect to emotion- specific discrete expression models in contrast to dimensional models of emotion encoding.},
author = {Wallbott, Harald G.},
doi = {10.1002/(SICI)1099-0992(1998110)28:6<879::AID-EJSP901>3.0.CO;2-W},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Wallbott - 1998 - Bodily Expression of Emotion.pdf:pdf},
isbn = {0046-2772},
issn = {0046-2772},
journal = {European Journal of Social Psychology},
number = {6},
pages = {879--896},
title = {{Bodily Expression of Emotion}},
url = {http://doi.wiley.com/10.1002/(SICI)1099-0992(1998110)28:6{\%}3C879::AID-EJSP901{\%}3E3.0.CO;2-W},
volume = {28},
year = {1998}
}
@article{Heni2015,
abstract = {Since the emergence of smartphones, there is a noticeable shift in our daily lives from desktop use to mobile use while maintaining interactivity, improving the use of these devices. Mobile gaming is an area that can provide a very personalized way to enjoy life for smartphone users. Today, while smart devices have the ability to recognize facial expressions, robust recognition of facial expressions in real time remains a challenge due to difficulties in accurately extracting the most pertinent emotional characteristics. A valuable objective is to define a computerized system facilitating the design and implementation of behavior detection on mobile devices for various applications. Our work aims to detect emotions through the face recognition of a smartphone user in order to synthesize his behavior while playing. {\textcopyright} 2015 IEEE.},
author = {Heni, Nazih and Hamam, Habib},
doi = {10.1109/CCECE.2015.7129456},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Heni, Hamam - 2015 - Facial emotion detection of smartphone games users.pdf:pdf},
isbn = {9781479958290},
issn = {08407789},
journal = {Canadian Conference on Electrical and Computer Engineering},
keywords = {behavior detection,facial expressions,recognition},
number = {June},
pages = {1243--1247},
title = {{Facial emotion detection of smartphone games users}},
volume = {2015-June},
year = {2015}
}
@article{Soleymani2015,
abstract = {This paper presents a user-independent emotion recognition method with the goal of recovering affective tags for videos using electroencephalogram (EEG), pupillary response and gaze distance. We first selected 20 video clips with extrinsic emotional content from movies and online resources. Then, EEG responses and eye gaze data were recorded from 24 participants while watching emotional video clips. Ground truth was defined based on the median arousal and valence scores given to clips in a preliminary study using an online questionnaire. Based on the participants' responses, three classes for each dimension were defined. The arousal classes were calm, medium aroused, and activated and the valence classes were unpleasant, neutral, and pleasant. One of the three affective labels of either valence or arousal was determined by classification of bodily responses. A one-participant-out cross validation was employed to investigate the classification performance in a user-independent approach. The best classification accuracies of 68.5 percent for three labels of valence and 76.4 percent for three labels of arousal were obtained using a modality fusion strategy and a support vector machine. The results over a population of 24 participants demonstrate that user-independent emotion recognition can outperform individual self-reports for arousal assessments and do not underperform for valence assessments.},
author = {Soleymani, Mohammad and Pantic, Maja and Pun, Thierry},
doi = {10.1109/ACII.2015.7344615},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Soleymani, Pantic, Pun - 2015 - Multimodal emotion recognition in response to videos (Extended abstract).pdf:pdf},
isbn = {9781479999538},
issn = {19493045},
journal = {2015 International Conference on Affective Computing and Intelligent Interaction, ACII 2015},
keywords = {EEG signals,emotions,implicit tagging},
number = {2},
pages = {491--497},
title = {{Multimodal emotion recognition in response to videos (Extended abstract)}},
volume = {3},
year = {2015}
}
@article{H.R.LvZ.L.LinW.J.Yin2008,
abstract = {This paper describes a new approach to emotion recognition based on pressure sensor keyboards. The pressure sensor keyboard is a new product that occurs in the market recently, which produces a pressure sequence when keystroke occurs. The analysis of the pressure sequence should be a novel research area. It has been used for identity verification in our previous research. In this paper, we use the pressure sequence for emotion recognition. Three methods (global features of pressure sequences, dynamic time warping and traditional keystroke dynamics) are proposed for the emotion recognition task; then we combined the three methods together using a classifier fusion technique. Several experiments were performed on a database containing 3000 samples (from 50 individuals, including six emotions: neutral, anger, fear, happiness, sadness and surprise) and the best result were achieved utilizing all the method, obtaining an overall accuracy of 93.4{\%}. Our technique of emotion recognition has been used for intelligent game controlling and several other applications.},
author = {{H. R. Lv, Z. L. Lin, W. J. Yin}, J. Dong},
doi = {10.1109/ICME.2008.4607628},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/H. R. Lv, Z. L. Lin, W. J. Yin - 2008 - Emotion recognition based on pressure sensor keyboards.pdf:pdf},
isbn = {978-1-4244-2570-9},
journal = {2008 IEEE International Conference on Multimedia and Expo},
pages = {1089--1092},
title = {{Emotion recognition based on pressure sensor keyboards}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4607628},
year = {2008}
}
@article{Yarkoni2011,
abstract = {The rapid growth of the literature on neuroimaging in humans has led to major advances in our understanding of human brain function but has also made it increasingly difficult to aggregate and synthesize neuroimaging findings. Here we describe and validate an automated brain-mapping framework that uses text-mining, meta-analysis and machine-learning techniques to generate a large database of mappings between neural and cognitive states. We show that our approach can be used to automatically conduct large-scale, high-quality neuroimaging meta-analyses, address long-standing inferential problems in the neuroimaging literature and support accurate 'decoding' of broad cognitive states from brain activity in both entire studies and individual human subjects. Collectively, our results have validated a powerful and generative framework for synthesizing human neuroimaging data on an unprecedented scale.},
author = {Yarkoni, Tal and Poldrack, Russell A and Nichols, Thomas E and {Van Essen}, David C and Wager, Tor D},
doi = {10.1038/nmeth.1635},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Yarkoni et al. - 2011 - Large-scale automated synthesis of human functional neuroimaging data.pdf:pdf},
isbn = {1662-5196},
issn = {1548-7091},
journal = {Nature Methods},
number = {8},
pages = {665--670},
pmid = {21706013},
title = {{Large-scale automated synthesis of human functional neuroimaging data}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.1635},
volume = {8},
year = {2011}
}
@article{Reeves1998,
abstract = {Fresh evidence of human gullibility never fails to entertain. Stanford professors Reeves and Nass provide plenty of cocktail-party ammunition with findings from 35 laboratory experiments demonstrating how even technologically sophisticated people treat boxes of circuitry as if they were other human beings. People are polite to computers, respond to praise from them and view them as teammates. They like computers with personalities similar to their own, find masculine-sounding computers extroverted, driven and intelligent while they judge feminine-sounding computers knowledgeable about love and relationships. Viewers rate content on a TV embellished with the label "specialist" superior to identical content on a TV labeled "generalist" (they even found the picture clearer on the "specialist" box). Reeves and Nass, who combine expertise in fine arts, communications, math, sociology, television and computers, were consultants to the world's foremost software corporation on the creation of the Microsoft Bob software package. Not surprisingly, their breezy tone and emphasis on the benign practical applications of their discoveries give their discussion an optimistic bias. Why not make media easier to use and more fun? Yet, their more important contribution may lie in alerting us to specific media dangers. The evidence of our suggestibility offers particularly powerful new arguments for monitoring children's television. And if the mere number of rapid-fire visual cuts in political advertisements really correlates with an impression of honesty, intelligence and sincerity, the more viewers who are put on guard, the better.},
author = {Reeves, Byron and Nass, Clifford},
doi = {10.1109/MSPEC.1997.576013},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Reeves, Nass - 1998 - The media equation.pdf:pdf},
isbn = {1575860538},
issn = {0018-9235},
journal = {The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places},
pages = {19--36},
pmid = {186262},
title = {{The media equation}},
volume = {34},
year = {1998}
}
@article{Neuroscience2012,
author = {Neuroscience, Human and Brown, Stephen B R E and Steenbergen, Henk Van and Guido, P and Band, H and Rover, Mischa De and Nieuwenhuis, Sander},
doi = {10.3389/fnhum.2012.00033},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Neuroscience et al. - 2012 - Functional significance of the emotion-related late positive potential.pdf:pdf},
keywords = {ERP,LPP,emotion,erp,global inhibition,late positive potential,lpp,perception},
number = {February},
pages = {1--12},
title = {{Functional significance of the emotion-related late positive potential}},
volume = {6},
year = {2012}
}
@misc{Ekman1969,
abstract = {Observers in both literate and preliterate cultures chose the predicted emotion for photographs of the face, although agreement was higher in the literate samples. These findings suggest that the pan-cultural element in facial displays of emotion is the association between facial muscular movements and discrete primary emotions, although cultures may still differ in what evokes an emotion, in rules for controlling the display of emotion, and in behavioral consequences.},
author = {Ekman, Paul and Sorenson, E Richard and Friesen, Wallace V},
booktitle = {Science},
doi = {10.1126/science.164.3875.86},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Ekman, Sorenson, Friesen - 1969 - Pan-Cultural Elements in Facial Displays of Emotion.pdf:pdf},
isbn = {0036-8075 (Print)},
issn = {0036-8075},
number = {3875},
pages = {86--88},
pmid = {5773719},
title = {{Pan-Cultural Elements in Facial Displays of Emotion}},
url = {http://science.sciencemag.org/content/164/3875/86.abstract},
volume = {164},
year = {1969}
}
@article{Sundstrom2005,
author = {Sundstr{\"{o}}m, Petra and St{\aa}hl, Anna and H{\"{o}}{\"{o}}k, Kristina},
file = {:Users/kevin/Library/Application Support/Mendeley Desktop/Downloaded/Sundstr{\"{o}}m, St{\aa}hl, H{\"{o}}{\"{o}}k - 2005 - A User-Centered Approach to Affective Interaction.pdf:pdf},
isbn = {3540296212},
issn = {03029743},
journal = {Affective Computing and Intelligent Interaction: First International Conference, ACII 2005, Beijing, China, October 22-24, 2005. Proceedings},
pages = {931--938},
title = {{A User-Centered Approach to Affective Interaction}},
year = {2005}
}
