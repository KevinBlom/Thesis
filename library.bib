Automatically generated by Mendeley Desktop 1.17.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Neuroscience2012,
author = {Neuroscience, Human and Brown, Stephen B R E and Steenbergen, Henk Van and Guido, P and Band, H and Rover, Mischa De and Nieuwenhuis, Sander},
doi = {10.3389/fnhum.2012.00033},
file = {:Users/Kevin/Downloads/fnhum-06-00033.pdf:pdf},
keywords = {ERP,LPP,emotion,erp,global inhibition,late positive potential,lpp,perception},
number = {February},
pages = {1--12},
title = {{Functional significance of the emotion-related late positive potential}},
volume = {6},
year = {2012}
}
@article{Coulson2004,
abstract = {ABSTRACT: A total of 176 computer-generated mannequin figures were produced from descriptions of postural expressions of emotion in order to investigate the attri- bution of emotion to static body postures. Each posture was rendered from 3 view- ing angles and presented to participants in a forced-decision task. Concordance rates for attributions of 6 emotions (anger, disgust, fear, happiness, sadness, and surprise) ranged from zero for many disgust postures to over 90 percent for some anger and sadness postures. Anatomical variables and viewing angle were shown to predict participants' responses. Analysis of the confusion matrix suggested a cir- cumplex solution with happiness and surprise sharing a similar position, and few confusions between the other four emotions. The means by which emotions may be attributed to static body postures are discussed, as are avenues for further research.},
author = {Coulson, Mark},
doi = {10.1023/B:JONB.0000023655.25550.be},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/art{\%}3A10.1023{\%}2FB{\%}3AJONB.0000023655.25550.be.pdf:pdf},
isbn = {0191-5886},
issn = {0191-5886},
journal = {Journal of Nonverbal Behavior},
keywords = {and,and communication of emo-,emotion,expression,facial and vocal expression,posture,posture in the expression,relation of research into,s the expression of,since publication of darwin,the emotions in man,the role of body,tion remains the poor},
number = {2},
pages = {117--139},
pmid = {4869},
title = {{Attributing emotion to static body postures: recongition accuracy, confusion and view point dependence}},
volume = {28},
year = {2004}
}
@article{Silva1997,
author = {Silva, Liyanage C D E and Miyasato, I Tsutomu},
doi = {10.1109/SMC.2015.387},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/00647126.pdf:pdf},
isbn = {0780336763},
number = {September},
pages = {9--12},
title = {{Facial Emotion Recognition Using}},
year = {1997}
}
@article{Ververidis2004,
abstract = {Our purpose is to design a useful tool which can be used in psy- chology to automatically classify utterances into five emotional states such as anger, happiness, neutral, sadness, and surprise. The major contribution of the paper is to rate the discriminating capa- bility of a set of features for emotional speech recognition. A total of 87 features has been calculated over 500 utterances from the Danish Emotional Speech database. The Sequential Forward Se- lection method (SFS) has been used in order to discover a set of 5 to 10 features which are able to classify the utterances in the best way. The criterion used in SFS is the crossvalidated correct clas- sification score of one of the following classifiers: nearest mean and Bayes classifier where class pdfs are approximated via Parzen windows or modelled as Gaussians. After selecting the 5 best fea- tures, we reduce the dimensionality to two by applying principal component analysis. The result is a 51.6{\%} ± 3{\%} correct classifi- cation rate at 95{\%} confidence interval for the five aforementioned emotions, whereas a random classification would give a correct classification rate of 20{\%}. Furthermore, we find out those two- class emotion recognition problems whose error rates contribute heavily to the average error and we indicate that a possible reduc- tion of the error rates reported in this paper would be achieved by employing two-class classifiers and combining them..},
author = {Ververidis, Dimitrios and Kotropoulos, Constantine and Ioannis, Pitas},
doi = {10.1109/ICASSP.2004.1326055},
file = {:Users/Kevin/Downloads/01326055.pdf:pdf},
isbn = {0-7803-8484-9},
issn = {15206149},
journal = {Artificial Intelligence and Information Analysis Laboratory},
pages = {593-- 596},
title = {{Automatic Emotional Speech Classification}},
year = {2004}
}
@article{Posner2005,
abstract = {The circumplex model of affect proposes that all affective states arise from cognitive interpretations of core neural sensations that are the product of two independent neurophysiological systems. This model stands in contrast to theories of basic emotions, which posit that a discrete and independent neural system subserves every emotion. We propose that basic emotion theories no longer explain adequately the vast number of empirical observations from studies in affective neuroscience, and we suggest that a conceptual shift is needed in the empirical approaches taken to the study of emotion and affective psychopathologies. The circumplex model of affect is more consistent with many recent findings from behavioral, cognitive neuroscience, neuroimaging, and developmental studies of affect. Moreover, the model offers new theoretical and empirical approaches to studying the development of affective disorders as well as the genetic and cognitive underpinnings of affective processing within the central nervous system.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Posner, Jonathan and Russell, James A and Peterson, Bradley S},
doi = {10.1017/S0954579405050340},
eprint = {NIHMS150003},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/div-class-title-the-circumplex-model-of-affect-an-integrative-approach-to-affective-neuroscience-cognitive-development-and-psychopathology-div.pdf:pdf},
isbn = {0954-5794 (Print) 0954-5794 (Linking)},
issn = {0954-5794},
journal = {Development and psychopathology},
number = {3},
pages = {715--34},
pmid = {16262989},
title = {{The circumplex model of affect: an integrative approach to affective neuroscience, cognitive development, and psychopathology.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16262989},
volume = {17},
year = {2005}
}
@article{Shah2015,
abstract = {The role of affect and emotion in interactive system design is an active and recent research area. The aim is to make systems more responsive to user's needs and expectations. The first step towards affective interaction is to recognize user's emotional state. Literature contains many works on emotion recognition. In those works, facial muscle movement, gestures, postures and physiological signals were used for recognition. The methods are computation intensive and require extra hardware (e.g., sensors and wires). In this work, we propose a simpler model to predict the affective state of a touch screen user. The prediction is done based on the user's touch input, namely the finger strokes. We defined seven features based on the strokes. A linear combination of these features is proposed as the predictor, which can predict a user's affective state into one of the three states: positive (happy, excited and elated), negative (sad, anger, fear, disgust) and neutral (calm, relaxed and contented). The model alleviates the need for extra setup as well as extensive computation, making it suitable for implementation on mobile devices with limited resources. The model is developed and validated with empirical data involving 57 participants performing 7 touch input tasks. The validation study demonstrates a high prediction accuracy of 90.47 {\%}. The proposed model and its empirical development and validation are described in this paper.},
author = {Shah, Sachin and Teja, J. Narasimha and Bhattacharya, Samit},
doi = {10.1186/s40166-015-0013-z},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/art{\%}3A10.1186{\%}2Fs40166-015-0013-z.pdf:pdf},
issn = {2194-0827},
journal = {Journal of Interaction Science},
keywords = {emotional state touch screen,linear regression empirical study,strike and tap features},
number = {1},
pages = {6},
publisher = {Journal of Interaction Science},
title = {{Towards affective touch interaction: predicting mobile user emotion from finger strokes}},
url = {http://www.journalofinteractionscience.com/content/3/1/6},
volume = {3},
year = {2015}
}
@article{H.R.LvZ.L.LinW.J.Yin2008,
abstract = {This paper describes a new approach to emotion recognition based on pressure sensor keyboards. The pressure sensor keyboard is a new product that occurs in the market recently, which produces a pressure sequence when keystroke occurs. The analysis of the pressure sequence should be a novel research area. It has been used for identity verification in our previous research. In this paper, we use the pressure sequence for emotion recognition. Three methods (global features of pressure sequences, dynamic time warping and traditional keystroke dynamics) are proposed for the emotion recognition task; then we combined the three methods together using a classifier fusion technique. Several experiments were performed on a database containing 3000 samples (from 50 individuals, including six emotions: neutral, anger, fear, happiness, sadness and surprise) and the best result were achieved utilizing all the method, obtaining an overall accuracy of 93.4{\%}. Our technique of emotion recognition has been used for intelligent game controlling and several other applications.},
author = {{H. R. Lv, Z. L. Lin, W. J. Yin}, J. Dong},
doi = {10.1109/ICME.2008.4607628},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/lv08.pdf:pdf},
isbn = {978-1-4244-2570-9},
journal = {2008 IEEE International Conference on Multimedia and Expo},
pages = {1089--1092},
title = {{Emotion recognition based on pressure sensor keyboards}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4607628},
year = {2008}
}
@article{Picard1995,
abstract = {Computers are beginning to acquire the ability to ex- press and recognize affect, and may soon be given the ability to “have emotions.” The essential role of emotion in both human cognition and perception, as demonstrated by recent neurological studies, indi- cates that affective computers should not only pro- vide better performance in assisting humans, but also might enhance computers' abilities to make de- cisions. This paper presents and discusses key issues in “affective computing,” computing that relates to, arises from, or influences emotions. Models are sug- gested for computer recognition of human emotion, and new applications are presented for computer- assisted learning, perceptual information retrieval, arts and entertainment, and human health and inter- action. Affective computing, coupled with new wear- able computers, will also provide the ability to gather new data necessary for advances in emotion and cog- nition theory.},
author = {Picard, Rosalind W.},
doi = {10.1007/BF01238028},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/1995{\_}Affective computing{\_}Picard.pdf:pdf},
isbn = {0262161702},
issn = {09269630},
journal = {MIT press},
number = {321},
pages = {1--16},
pmid = {18487783},
title = {{Affective Computing}},
url = {papers3://publication/uuid/9C02FCAE-FE2E-4D2C-9707-766804777DC9},
year = {1995}
}
@article{LiuJ2007,
author = {Liu, Jia and Chen, Chun and Bu, Jiajun and You, Mingyu and Tao, Jianhua},
file = {:Users/Kevin/Downloads/04284821.pdf:pdf},
isbn = {1424410177},
pages = {999--1002},
title = {{SPEECH EMOTION RECOGNITION USING AN ENHANCED CO-TRAINING ALGORITHM Jia Liu , Chun Chen , Jiajun Bu , Mingyu You College of Computer Science Zhejiang University National Lab of Pattern Recognition Chinese Academy of Sciences}},
year = {2007}
}
@article{Soleymani2015,
abstract = {This paper presents a user-independent emotion recognition method with the goal of recovering affective tags for videos using electroencephalogram (EEG), pupillary response and gaze distance. We first selected 20 video clips with extrinsic emotional content from movies and online resources. Then, EEG responses and eye gaze data were recorded from 24 participants while watching emotional video clips. Ground truth was defined based on the median arousal and valence scores given to clips in a preliminary study using an online questionnaire. Based on the participants' responses, three classes for each dimension were defined. The arousal classes were calm, medium aroused, and activated and the valence classes were unpleasant, neutral, and pleasant. One of the three affective labels of either valence or arousal was determined by classification of bodily responses. A one-participant-out cross validation was employed to investigate the classification performance in a user-independent approach. The best classification accuracies of 68.5 percent for three labels of valence and 76.4 percent for three labels of arousal were obtained using a modality fusion strategy and a support vector machine. The results over a population of 24 participants demonstrate that user-independent emotion recognition can outperform individual self-reports for arousal assessments and do not underperform for valence assessments.},
author = {Soleymani, Mohammad and Pantic, Maja and Pun, Thierry},
doi = {10.1109/ACII.2015.7344615},
file = {:Users/Kevin/Downloads/06095505.pdf:pdf},
isbn = {9781479999538},
issn = {19493045},
journal = {2015 International Conference on Affective Computing and Intelligent Interaction, ACII 2015},
keywords = {EEG signals,emotions,implicit tagging},
number = {2},
pages = {491--497},
title = {{Multimodal emotion recognition in response to videos (Extended abstract)}},
volume = {3},
year = {2015}
}
@misc{Ekman1969,
abstract = {Observers in both literate and preliterate cultures chose the predicted emotion for photographs of the face, although agreement was higher in the literate samples. These findings suggest that the pan-cultural element in facial displays of emotion is the association between facial muscular movements and discrete primary emotions, although cultures may still differ in what evokes an emotion, in rules for controlling the display of emotion, and in behavioral consequences.},
author = {Ekman, Paul and Sorenson, E Richard and Friesen, Wallace V},
booktitle = {Science},
doi = {10.1126/science.164.3875.86},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/pan-cultural{\_}elements{\_}in{\_}facial{\_}displays{\_}of{\_}emotions.pdf:pdf},
isbn = {0036-8075 (Print)},
issn = {0036-8075},
number = {3875},
pages = {86--88},
pmid = {5773719},
title = {{Pan-Cultural Elements in Facial Displays of Emotion}},
url = {http://science.sciencemag.org/content/164/3875/86.abstract},
volume = {164},
year = {1969}
}
@article{Mauss2009,
abstract = {A consensual, componential model of emotions conceptualises them as experiential, physiological, and behavioural responses to personally meaningful stimuli. The present review examines this model in terms of whether different types of emotion-evocative stimuli are associated with discrete and invariant patterns of responding in each response system, how such responses are structured, and if such responses converge across different response systems. Across response systems, the bulk of the available evidence favours the idea that measures of emotional responding reflect dimensions rather than discrete states. In addition, experiential, physiological, and behavioural response systems are associated with unique sources of variance, which in turn limits the magnitude of convergence across measures. Accordingly, the authors suggest that there is no "gold standard" measure of emotional responding. Rather, experiential, physiological, and behavioural measures are all relevant to understanding emotion and cannot be assumed to be interchangeable.},
author = {Mauss, Iris B and Robinson, Michael D},
doi = {10.1080/02699930802204677},
file = {:Users/Kevin/Library/Application Support/Mendeley Desktop/Downloaded/Mauss, Robinson - 2009 - Measures of emotion A review.pdf:pdf},
isbn = {1464-0600 (Electronic)$\backslash$r0269-9931 (Linking)},
issn = {1464-0600},
journal = {Cognition and Emotion},
keywords = {Autonomic nervous system,Behaviour,Central nervous system,Emotion,Measurement,Self-report,Specificity,Startle modulation},
number = {2},
pages = {209--237},
pmid = {19809584},
title = {{Measures of emotion: A review.}},
url = {http://www.tandfonline.com/doi/abs/10.1080/02699930802204677},
volume = {23},
year = {2009}
}
@article{Lee2012,
abstract = {Awareness of the emotion of those who communicate with others is a fundamental challenge in building affective intelligent systems. Emotion is a complex state of the mind influenced by external events, physiological changes, or relationships with others. Because emotions can represent a user's internal context or intention, researchers suggested various methods to measure the user's emotions from analysis of physiological signals, facial expressions, or voice. However, existing methods have practical limitations to be used with consumer devices, such as smartphones; they may cause inconvenience to users and require special equipment such as a skin conductance sensor. Our approach is to recognize emotions of the user by inconspicuously collecting and analyzing user-generated data from different types of sensors on the smartphone. To achieve this, we adopted a machine learning approach to gather, analyze and classify device usage patterns, and developed a social network service client for Android smartphones which unobtrusively find various behavioral patterns and the current context of users. Also, we conducted a pilot study to gather real-world data which imply various behaviors and situations of a participant in her/his everyday life. From these data, we extracted 10 features and applied them to build a Bayesian Network classifier for emotion recognition. Experimental results show that our system can classify user emotions into 7 classes such as happiness, surprise, anger, disgust, sadness, fear, and neutral with a surprisingly high accuracy. The proposed system applied to a smartphone demonstrated the feasibility of an unobtrusive emotion recognition approach and a user scenario for emotion-oriented social communication between users.},
author = {Lee, Hosub and Choi, Young Sang and Lee, Sunjae and Park, I. P.},
doi = {10.1109/CCNC.2012.6181098},
file = {:Users/Kevin/Downloads/01ed0ec002b81b4882b17d435cedbe0a0f7e.pdf:pdf},
isbn = {9781457720710},
journal = {2012 IEEE Consumer Communications and Networking Conference, CCNC'2012},
keywords = {Affective computing,Computer mediated communication,Emotion recognition,Machine intelligence,Supervised learning},
pages = {260--264},
title = {{Towards unobtrusive emotion recognition for affective social communication}},
year = {2012}
}
@article{Wallbott1998,
abstract = {The question whether body movements and body postures are indicative of specific emotions is a matter of debate. While some studies have found evidence for specific body movements accompanying specific emotions, others indicate that movement behavior (aside from facial expression) may be only indicative of the quantity (intensity) of emotion, but not of its quality. The study reported here is an attempt to demonstrate that body movements and postures to some degree are specific for certain emotions. A sample of 224 video takes, in which actors and actresses portrayed the emotions of elated joy, happiness, sadness, despair, fear, terror, cold anger, hot anger, disgust, contempt, shame, guilt, pride, and boredom via a scenario approach, was analyzed using coding schemata for the analysis of body movements and postures. Results indicate that some emotion-specific movement and posture characteristics seem to exist, but that for body movements dierences between emotions can be partly explained by the dimension of activation. While encoder (actor) diferences are rather pronounced with respect to specific movement and posture habits, these dierences are largely independent from the emotion-specific dierences found. The results are discussed with respect to emotion- specific discrete expression models in contrast to dimensional models of emotion encoding.},
author = {Wallbott, Harald G.},
doi = {10.1002/(SICI)1099-0992(1998110)28:6<879::AID-EJSP901>3.0.CO;2-W},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/Wallbott, 1998, bodiliy espression of emotion.pdf:pdf},
isbn = {0046-2772},
issn = {0046-2772},
journal = {European Journal of Social Psychology},
number = {6},
pages = {879--896},
title = {{Bodily Expression of Emotion}},
url = {http://doi.wiley.com/10.1002/(SICI)1099-0992(1998110)28:6{\%}3C879::AID-EJSP901{\%}3E3.0.CO;2-W},
volume = {28},
year = {1998}
}
@article{Likamwa2013,
abstract = {We report a first-of-its-kind smartphone software system, MoodScope, which infers the mood of its user based on how the smartphone is used. Compared to smartphone sensors that measure acceleration, light, and other physical properties, MoodScope is a "sensor" that measures the mental state of the user and provides mood as an important input to context-aware computing. We run a formative statistical mood study with smartphone-logged data collected from 32 participants over two months. Through the study, we find that by analyzing communication history and application usage patterns, we can statistically infer a user's daily mood average with an initial accuracy of 66{\%}, which gradu-ally improves to an accuracy of 93{\%} after a two-month personal-ized training period. Motivated by these results, we build a service, MoodScope, which analyzes usage history to act as a sensor of the user's mood. We provide a MoodScope API for developers to use our system to create mood-enabled applications. We further create and deploy a mood-sharing social application.},
author = {Likamwa, Robert and Liu, Yunxin and Lane, Nicholas D. and Zhong, Lin},
doi = {10.1145/2462456.2464449},
file = {:Users/Kevin/Downloads/p389-likamwa.pdf:pdf},
isbn = {9781450316729},
journal = {MobiSys '13 Proceeding of the 11th annual international conference on Mobile systems, applications, and services},
keywords = {affective computing,machine learning,mood,smartphone usage},
number = {April},
pages = {389--402},
title = {{MoodScope: Building a Mood Sensor from Smartphone Usage Patterns}},
year = {2013}
}
@article{Gao2012,
abstract = {The increasing number of people playing games on touch-screen mobile phones raises the question of whether touch behaviors reflect players' emotional states. This prospect would not only be a valuable eval- uation indicator for game designers, but also for real-time personalization of the game experience. Psychol- ogy studies on acted touch behavior show the existence of discriminative affective profiles. In this article, finger-stroke features during gameplay on an iPod were extracted and their discriminative power analyzed. Machine learning algorithms were used to build systems for automatically discriminating between four emo- tional states (Excited, Relaxed, Frustrated, Bored), two levels of arousal and two levels of valence. Accuracy reached between 69{\%} and 77{\%} for the four emotional states, and higher results (∼89{\%}) were obtained for discriminating between two levels of arousal and two levels of valence. We conclude by discussing the factors relevant to the generalization of the results to applications other than games.},
author = {Gao, Yuan and Bianchi-Berthouze, Nadia and Meng, Hongying},
doi = {10.1145/2395131.2395138},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/a31-gao.pdf:pdf},
isbn = {10730516 (ISSN)},
issn = {10730516},
journal = {ACM Transactions on Computer-Human Interaction},
number = {4},
pages = {1--30},
title = {{What Does Touch Tell Us about Emotions in Touchscreen-Based Gameplay?}},
url = {http://dl.acm.org/citation.cfm?doid=2395131.2395138},
volume = {19},
year = {2012}
}
@article{Singh2015,
abstract = {In this article, we describe the Differential Equations and Optimization Environment (DOpElib). DOpElib is a software library that provides a unified interface to high level algorithms such as time-stepping methods, nonlinear solvers and optimization routines. This structure ensures that, first of all, the user is only required to write those sections of code that are specific to the considered problem. Second, the exchange of parts of the used routines is possible with only a few lines of code to change instead of large reimplementations. The article illustrates the design principles and various features of DOpElib and provides some numerical results as demonstration for the versatility of the software},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.07526v1},
author = {Singh, Munindar P.},
doi = {10.1145/0000000.0000000},
eprint = {arXiv:1502.07526v1},
file = {:Users/Kevin/Downloads/18a436d92b0bfe3b53ea6fe023649e775376.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
number = {March},
pages = {4207--4211},
pmid = {1000285845},
title = {{Norms as a basis for governing sociotechnical systems}},
volume = {2015-Janua},
year = {2015}
}
@misc{Ekman1992,
abstract = {Emotions are viewed as having evolved through their adaptive value in dealing with fundamental life-tasks. Each emotion has unique features: signal, physiology, and antecedent events. Each emotion also has characteristics in common with other emotions: rapid onset, short duration, unbidden occurrence, automatic appraisal, and coherence among responses. These shared and unique characteristics are the product of our evolution, and distinguish emotions from other affective pheonomena.},
archivePrefix = {arXiv},
arxivId = {a},
author = {Ekman, Paul},
booktitle = {Cognition {\&} Emotion},
doi = {10.1080/02699939208411068},
eprint = {a},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/An-Argument-For-Basic-Emotions.pdf:pdf},
isbn = {0269-9931},
issn = {0269-9931},
number = {3},
pages = {169--200},
pmid = {665},
title = {{An argument for basic emotions}},
volume = {6},
year = {1992}
}
@article{Wioleta2013,
abstract = {review},
author = {Wioleta, Szwoch},
doi = {10.1109/HSI.2013.6577880},
file = {:Users/Kevin/stack/Master Thesis/Thesis Design/references/06577880.pdf:pdf},
isbn = {9781467356374},
issn = {2158-2246},
journal = {Human System Interaction (HSI), 2013 The 6th {\ldots}},
keywords = {4,affective computing,applications of emotion recognition,emotion recognition,physiological signals,possible,so there are many,their emotions explicitly,using physiological},
pages = {556--561},
title = {{Using physiological signals for emotion recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6577880},
year = {2013}
}
