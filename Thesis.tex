% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
  \pdfpagewidth=8.5truein
  \pdfpageheight=11truein

% Remove copyright space
\makeatletter
\def\@copyrightspace{\relax}
\makeatother

% Packages
\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}

\begin{document}

\title{Detecting emotion with a pressure sensitive touchscreen}
\subtitle{[...]}


\numberofauthors{1} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%{}
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor Kevin Blom\\
       \affaddr{University of Amsterdam}\\
       \affaddr{Science Park 904}\\
       \email{xxxxxx@xxxxxx.xx}
}

\maketitle

\begin{abstract}
	
\end{abstract}

\section{Introduction} % (fold)
\label{sec:introduction}
Affective computing as introduced by Picard\cite{Picard1995} in 1995 lays a foundation for computers and technology to incorporate the recognition and expression of emotions. It can provide better performance when assisting humans or enhance the computers ability to make decisions. It does not have the goal of making computers more human-like, but it is more practical in nature; make computers function with intelligence and sensitivity towards its users\cite{Picard1997}.  According to Shah et al.\cite{Shah2015} there are two general models to represent emotion; discrete and continuous. The discrete model represent emotions that are measurable and physiologically distinct like angry, sad, happy, etc. \cite{Ekman1992} The continuous model represents emotions on a two-dimensional scale, where one axis represents \textit{valence} and the other \textit{arousal} \cite{Posner2005}. Mauss et al. \cite{Mauss2009} suggest that using a dimensional framework is a better option when capturing emotion, relative to discrete frameworks. Since, the measuring of emotion has been a subject of research and several different angles have been discovered to approach it.

\subsection{Physiological detection} % (fold)
\label{sub:physiology}
One angle uses physiological signals of the human body to measure and detect emotion. In a review by Wioleta\cite{Wioleta2013}, eight studies were collected that measure emotion using one or more physiological signals combined. These signals are \textit{EEG, skin conductance, blood volume pulse, temperature, heart rate, blood pressure, respiration, EMG,} and \textit{ECG}. Most of these physiological signals have the drawback that they need specialized sensors attached to the body, making unobtrusive measurements difficult. With the recent rise of smart wearables, heart rate is one of the signals that is more readily available to use in applications on smart devices.
% subsection physiology (end)

\subsection{Facial detection} % (fold)
\label{sub:facial_detection}
Facial detection of emotion incorporates the measurement of facial muscle movement, voice or speech \cite{Ververidis2004}, and also includes the eye as point of detection, i.e. movement, blinking, and pupil dilation \cite{Soleymani2015}. By connecting facial muscle movement to visual display of emotions, Ekman et al. \cite{Ekman1969} conclude with a basic set of six mutually exclusive emotions that could be recognized. Expanding, De Silva et al. \cite{Silva1997} found that several emotions are expressed by either visual or auditory cues, or both, meaning that some emotions can be recognized by visual cues alone, auditory cues alone, or need a combination of both to be detected accurately.
% subsection facial_emotion_detection (end)

\subsection{Posture/gestures emotion detection}
Other means of detection emotions involve the tracking and interpretation of posture and gesture. Wallbott et al. \cite{Wallbott1998} concluded in 1998 that there are, in some cases, distinctive patterns of movement and postural behavior that have a strong correlation to emotions. In other cases, they mention that in absence of patterns there are still distinctive features from which emotion could be inferred. Coulson et al. \cite{Coulson2004} researched static body postures and the recognition of emotions from these body postures by participants. It showed that disgust is a tough emotion to recognize but anger and sadness had over 90\% correct detection rates. Furthermore, happiness and surprise were two emotions that were often confused. 

\subsection{Practical applications} % (fold)
\label{sub:practical_applications}
Looking at a more practical and applied side of emotion detection, Gao et al. \cite{Gao2012} used touchscreen devices, where the application of gestures on touch screens was successfully linked to emotional states with the use of a game. The emotional states that were tested for are: excited, relaxed, frustrated and bored, and accuracy of detection reached at minimum 69\%. However, the research of Gao et al. was limited to gestures and did not incorporate data from taps. Furthermore, Lv et al. \cite{H.R.LvZ.L.LinW.J.Yin2008} have created means to detect emotion from keyboard pressure using feature extraction. This indicates that the use of a keyboard on a touch screen could also be used as means of detecting emotion, but one must keep in mind that a regular keyboard is not fully comparable to a touchscreen keyboard. It lays flat on a desk, and is often typed upon with more than one or two fingers, which means that the pressure exerted on the keyboard is likely not directly correlated with the pressure on a touchscreen keyboard. Moreover, Lee et al \cite{Lee2012} propose an unobtrusive way of detecting emotion by analyzing smartphone usage patterns (not unlike LiKamWa et al. \cite{Likamwa2013}) and social network status updates. However, this required that the user would post status updates through independently developed social networking applications, that are not officially supported by the social networks themselves.

% subsection practical_applications (end)

\subsection{Research Question}
From the related work can be concluded that most types of detection of emotions are invasive, either requiring constant monitoring, possibly with sensors attached to the body, or by constant recording of audio and visual data. The touch screen is a technology a lot of people interact with every day, where they deliberately choose to participate in those interactions. Using touch screen presses as indicators for emotional state could be an unobtrusive way of detecting emotion without the need for constant monitoring. With the introduction of pressure sensitive touchscreens in recent smart devices, an interesting new sensor is added to the plethora of sensors already available. Subsequently, this leads to the following research question:\\

\textit{Can pressure sensitive touch screen devices be used to tell more about the mood of the user?}\\
% \begin{itemize}
% 	\item Description of background.
% 	\item Explanation of why research was necessary.
% 	\item Description of how research will be undertaken.
% \end{itemize}

% section Introduction (end)

\section{Methods} % (fold)
\label{sec:methods}
In order to test for the correlation between taps on a touch screen and emotion, there has to be a standardized way of eliciting different emotions. Fortunately, the University of Florida has a photo set that has been thoroughly tested for emotional response on a two dimensional scale. Utilizing this photo set as a baseline, touch screen taps and their force can be compared to the emotional response.

\subsection{Emotional elicitation} % (fold)
\label{sub:emotional_elicitation}
Using a standardized photo set that has been thoroughly research for emotional response when showed to participants, a ground truth for emotion can be set. The \textbf{FOTOSET?} photo set uses the continuous model of representing emotions, i.e. the two-dimensional valence and arousal model. Every participant was shown 40 pictures that were randomly selected from the complete database.
% subsection emotional_elicitation (end)

\subsection{Tap detection} % (fold)
\label{sub:tap_detection}
Taps were detected on an Apple iPhone 6s device with a 3D touch screen running iOS 10.2.1. The pressure of taps can be registered on a floating point scale from 0.0 to 6.67. Every tap returns on average 6 registrations of force in chronological order, creating a graph of pressure-over-time for every tap. For every picture that was shown to participants, five taps were registered in order to capture more data specific on the elicted emotion.
% subsection tap_detection (end)

\begin{enumerate}
	\item 
\end{enumerate}


\begin{itemize}
	\item Overview of the research.
	\item Report of who took part and where.
	\item Report of what procedures were used.
	\item Report of what materials were used.
	\item Report of any statistical analysis used.
\end{itemize}

% section methods (end)

\section{Results} % (fold)
\label{sec:results}
\begin{itemize}
	\item Report of findings.
	\item Reference to any diagrams used.
\end{itemize}
% section results (end)

\section{Discussion} % (fold)
\label{sec:discussion}
\begin{itemize}
	\item Summary of main purpose of research.
	\item Review of most important findings.
	\item Evaluation of findings.
	\item Explanation of findings.
	\item Comparison with other researchers findings.
	\item Description of implications and recommendations.
\end{itemize}


% section discussion (end)

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{library,Alternative}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

\end{document}